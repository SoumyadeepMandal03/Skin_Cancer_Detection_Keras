{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZennonXD/Skin_Cancer_Detection_Keras/blob/main/skin___cancer_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2v8rKHjmrS9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "7184e75f-3a65-4c68-b03f-a1aa56cb43e6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/final_year_project/HAM10000_metadata.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-345959999808>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Read the csv file containing image names and corresponding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mskin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/final_year_project/HAM10000_metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/final_year_project/HAM10000_metadata.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd # type: ignore\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Dump all images into a folder and specify the path:\n",
        "data_dir = os.path.join(os.getcwd(), \"/content/drive/MyDrive/final_year_project/all images\")\n",
        "\n",
        "# Path to destination directory where we want subfolders\n",
        "dest_dir = os.path.join(os.getcwd(), \"/content/drive/MyDrive/final_year_project/reorganised\")\n",
        "\n",
        "# Read the csv file containing image names and corresponding labels\n",
        "skin = pd.read_csv('/content/drive/MyDrive/final_year_project/HAM10000/HAM10000_metadata.csv')\n",
        "print(skin['dx'].value_counts())\n",
        "\n",
        "labels = skin['dx'].unique().tolist()  # Extract labels into a list\n",
        "\n",
        "# Copy images to new folders\n",
        "for label in labels:\n",
        "    os.mkdir(os.path.join(dest_dir, str(label)))\n",
        "    label_images = skin[skin['dx'] == label]['image_id']\n",
        "    for image_id in label_images:\n",
        "        src_path = os.path.join(data_dir, f\"{image_id}.jpg\")\n",
        "        dest_path = os.path.join(dest_dir, str(label), f\"{image_id}.jpg\")\n",
        "        shutil.copyfile(src_path, dest_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TYarwThw4WRi",
        "outputId": "0912c3b0-d7e6-4fae-a1b7-2f4ba11b29f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "5qb9YakZ4PPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kH9NwICiWaq",
        "outputId": "541b7e14-ac42-4b71-8fdd-9728edf796b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras.utils.np_utils\n"
      ],
      "metadata": {
        "id": "WYEXVUMAitCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "np.random.seed(42)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.utils import to_categorical # used for converting labels to one-hot-encoding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "skin_df = pd.read_csv('/content/drive/MyDrive/final_year_project/HAM10000/HAM10000_metadata.csv')\n",
        "\n",
        "SIZE=32\n",
        "\n",
        "# label encoding to numeric values from text\n",
        "le = LabelEncoder()\n",
        "le.fit(skin_df['dx'])\n",
        "LabelEncoder()\n",
        "print(list(le.classes_))\n",
        "\n",
        "skin_df['label'] = le.transform(skin_df[\"dx\"])\n",
        "print(skin_df.sample(10))\n",
        "\n",
        "\n",
        "# Data distribution visualization\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "ax1 = fig.add_subplot(221)\n",
        "skin_df['dx'].value_counts().plot(kind='bar', ax=ax1)\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_title('Cell Type');\n",
        "\n",
        "ax2 = fig.add_subplot(222)\n",
        "skin_df['sex'].value_counts().plot(kind='bar', ax=ax2)\n",
        "ax2.set_ylabel('Count', size=15)\n",
        "ax2.set_title('Sex');\n",
        "\n",
        "ax3 = fig.add_subplot(223)\n",
        "skin_df['localization'].value_counts().plot(kind='bar')\n",
        "ax3.set_ylabel('Count',size=12)\n",
        "ax3.set_title('Localization')\n",
        "\n",
        "ax4 = fig.add_subplot(224)\n",
        "sample_age = skin_df[pd.notnull(skin_df['age'])]\n",
        "sns.distplot(sample_age['age'], fit=stats.norm, color='red');\n",
        "ax4.set_title('Age')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Distribution of data into various classes\n",
        "from sklearn.utils import resample\n",
        "print(skin_df['label'].value_counts())\n",
        "\n",
        "#Balance data.\n",
        "# Many ways to balance data... you can also try assigning weights during model.fit\n",
        "#Separate each classes, resample, and combine back into single dataframe\n",
        "\n",
        "df_0 = skin_df[skin_df['label'] == 0]\n",
        "df_1 = skin_df[skin_df['label'] == 1]\n",
        "df_2 = skin_df[skin_df['label'] == 2]\n",
        "df_3 = skin_df[skin_df['label'] == 3]\n",
        "df_4 = skin_df[skin_df['label'] == 4]\n",
        "df_5 = skin_df[skin_df['label'] == 5]\n",
        "df_6 = skin_df[skin_df['label'] == 6]\n",
        "\n",
        "n_samples=500\n",
        "df_0_balanced = resample(df_0, replace=True, n_samples=n_samples, random_state=42)\n",
        "df_1_balanced = resample(df_1, replace=True, n_samples=n_samples, random_state=42)\n",
        "df_2_balanced = resample(df_2, replace=True, n_samples=n_samples, random_state=42)\n",
        "df_3_balanced = resample(df_3, replace=True, n_samples=n_samples, random_state=42)\n",
        "df_4_balanced = resample(df_4, replace=True, n_samples=n_samples, random_state=42)\n",
        "df_5_balanced = resample(df_5, replace=True, n_samples=n_samples, random_state=42)\n",
        "df_6_balanced = resample(df_6, replace=True, n_samples=n_samples, random_state=42)\n",
        "\n",
        "#Combined back to a single dataframe\n",
        "skin_df_balanced = pd.concat([df_0_balanced, df_1_balanced,\n",
        "                              df_2_balanced, df_3_balanced,\n",
        "                              df_4_balanced, df_5_balanced, df_6_balanced])\n",
        "\n",
        "#Check the distribution. All classes should be balanced now.\n",
        "print(skin_df_balanced['label'].value_counts())\n",
        "\n",
        "\n",
        "#reading images based on image ID from the CSV file\n",
        "#This is the safest way to read images as it ensures the right image is read for the right ID\n",
        "image_path = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join('/content/drive/MyDrive/final_year_project/reorganised', '*', '*.jpg'))}\n",
        "\n",
        "#Define the path and add as a new column\n",
        "skin_df_balanced['path'] = skin_df['image_id'].map(image_path.get)\n",
        "#Use the path to read images.\n",
        "skin_df_balanced['image'] = skin_df_balanced['path'].map(lambda x: np.asarray(Image.open(x).resize((SIZE,SIZE))) if x is not None else None)\n",
        "\n",
        "\n",
        "n_samples = 5  # number of samples for plotting\n",
        "# Plotting\n",
        "fig, m_axs = plt.subplots(7, n_samples, figsize = (4*n_samples, 3*7))\n",
        "for n_axs, (type_name, type_rows) in zip(m_axs,\n",
        "                                         skin_df_balanced.sort_values(['dx']).groupby('dx')):\n",
        "    n_axs[0].set_title(type_name)\n",
        "    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state=1234).iterrows()):\n",
        "        if c_row['image'] is not None:\n",
        "            c_ax.imshow(c_row['image'].astype(np.float32) / 255.)\n",
        "            c_ax.axis('off')\n",
        "\n",
        "\n",
        "\n",
        "#Convert dataframe column of images into numpy array\n",
        "X = np.asarray(skin_df_balanced['image'].tolist())\n",
        "X[X == None] = 0\n",
        "X = X/255.  # Scale values to 0-1.\n",
        "Y=skin_df_balanced['label']  #Assign label values to Y\n",
        "Y_cat = to_categorical(Y, num_classes=7) #Convert to categorical as this is a multiclass classification problem\n",
        "#Split to training and testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.25, random_state=42)\n",
        "\n",
        "#Define the model.\n",
        "#I've used autokeras to find out the best model for this problem.\n",
        "\n",
        "\n",
        "num_classes = 7\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(256, (3, 3), activation=\"relu\", input_shape=(SIZE, SIZE, 3)))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3),activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3),activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
        "\n",
        "\n",
        "# Train\n",
        "#You can also use generator to use augmentation during training.\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size = batch_size,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "plt.plot(epochs, acc, 'y', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Prediction on test data\n",
        "y_pred = model.predict(x_test)\n",
        "# Convert predictions classes to one hot vectors\n",
        "y_pred_classes = np.argmax(y_pred, axis = 1)\n",
        "# Convert test data to one hot vectors\n",
        "y_true = np.argmax(y_test, axis = 1)\n",
        "\n",
        "#Print confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "sns.set(font_scale=1.6)\n",
        "sns.heatmap(cm, annot=True, linewidths=.3, ax=ax)\n",
        "\n",
        "\n",
        "#PLot fractional incorrect misclassifications\n",
        "incorr_fraction = 1 - np.diag(cm) / np.sum(cm, axis=1)\n",
        "plt.bar(np.arange(7), incorr_fraction)\n",
        "plt.xlabel('True Label')\n",
        "plt.ylabel('Fraction of incorrect predictions')\n"
      ],
      "metadata": {
        "id": "trMN6H_mc9Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autokeras\n"
      ],
      "metadata": {
        "id": "Fo6OuBDAobsf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}